In this repository, you can find the data, code, and figures related to the paper: AUTOLYCUS: Exploiting Explainable Artificial Intelligence (XAI) for Model Extraction Attacks against Interpretable Models (to be published in PETs 2024).

1. You can experiment on the proposed attack using 'Experiments.ipynb'.
2. Datasets can be included in the code directly or manually added to the 'data' folder. Make sure they follow a similar format to other datasets for explainer compatibility.
3. Some experimental result plots from the development stage can be found in 'LIME/plots' and 'SHAP/plots'.
4. 'requirements.txt' provides the libraries and their version numbers as used in our work.
5. 'packages_full.txt' gives the list of all packages in the environment in which we conducted our experiments.
6. 'mlp_plot.py' and 'multimodel_plot.py' scripts can be added to the notebook to reproduce your own results as shown in the paper.

We welcome suggested improvements for streamlining the code and enhancing the attack. For further questions about the code, email abdullahcaglar.oksuz@case.edu.
