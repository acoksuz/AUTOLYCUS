In this repository, you can find the data, code and figures related to the paper: AUTOLYCUS: Exploiting Explainable Artificial Intelligence (XAI)
for Model Extraction Attacks against Interpretable Models. (will be published on PETs 2024)

1. You can experiment on the attack using 'Experiments.ipynb'.
2. Datasets can be included in the code directly or manually added into 'data' folder. Make sure they follow a similar format with other datasets for explainer compatibility.
3. Some experimental result plots from the development stage can be found in 'LIME/plots' and 'SHAP/plots'.
4. 'requirements.txt' provides the libraries and their version numbers as we used.
5. 'packages_full.txt' gives the list of all packages in the environment we conducted our experiments on.
6. 'mlp_plot.py' and 'multimodel_plot.py' scripts can be added into notebook for reproducing your own results as in the paper.

We welcome suggested improvements for streamlining the code and improving the attack. For further questions about the code, email abdullahcaglar.oksuz@case.edu.
